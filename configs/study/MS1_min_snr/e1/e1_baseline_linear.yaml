schema: study/v1
study_name: min_snr
out_dir: "runs/"
seed: 1077
metric: val/fid

_experiment:
  name: "e1-baseline-linear-full-50k-50nfe"

device: null     # auto
deterministic: true
clean_run: false

data:
  dataset: "cifar10"
  subset: null      # full train
  batch_size: 4
  num_workers: 2
  shuffle: true

model:
  name: "unet_cifar32"
  # small CIFAR-10 UNet (same architecture as in noise-sched E1/E2/E7)

ema:
  enabled: true
  decay: 0.999      # strong EMA, as in E7

diffusion:
  enabled: true
  beta_schedule: "linear"   # baseline linear β
  # num_timesteps: 1000
  # ... (keep other diffusion settings as in E7 / default base)

optim:
  optimizer: adam
  lr: 1.0e-4

train:
  total_steps: 50000        # 50k steps
  grad_clip: 1.0
  amp: true

eval:
  quick: false

  grid:
    enabled: true
    every: 5000
    sampler: ddim
    nfe: 20
    n_samples: 36
    save_images: true

  kid:
    enabled: false
    every: 4000
    n_samples: 1024
    repeats: 3
    sampler: ddim
    nfe: 20

  fid_milestone:
    enabled: true
    every: 10000                  # FID at ~10k, 20k, 30k, 40k
    run_if_kid_improved_pct: 0.0  # ignore KID gate
    sampler: ddpm
    nfe: 50
    n_samples: 5000               # cheaper intermittent FID
    fid_stats: "stats/cifar10_inception_train.npz"

  final:
    enabled: true
    at_end: true
    sampler: ddpm
    nfe: 50
    n_samples: 10000              # prereg: FID@10k
    fid_stats: "stats/cifar10_inception_train.npz"

logging:
  enable: true
  backends: ["tensorboard","wandb"]
  log_every_n_steps: 100

  wandb:
    mode: online
    project: min_snr
    run_name: "e1-baseline-linear-full-50k-50nfe"
    tags: ["e1", "baseline", "linear", "50k", "50nfe", "min_snr"]
    notes: "e1: linear β baseline, 50k steps, EMA=0.999, eval NFE=50, 10k samples."

  tensorboard:
    flush_secs: 10