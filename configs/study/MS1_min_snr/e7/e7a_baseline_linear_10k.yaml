schema: study/v1
study_name: min_snr
out_dir: "runs/"
seed: 1077
metric: val/fid

_experiment:
  name: "e7a-baseline-linear-hutch-10k-50nfe"

device: null
deterministic: true
clean_run: false

data:
  dataset: "cifar10"
  subset: null
  batch_size: 4
  num_workers: 2
  shuffle: true

model:
  name: "unet_cifar32"
  # default UNetCifar32:
  # base_channels=32, channel_mults=[1,2,2,2], num_res_blocks=2, dropout=0.1, etc.
  # (no overrides here; E7a is the reference baseline)

ema:
  enabled: true
  decay: 0.999

diffusion:
  enabled: true
  beta_schedule: "linear"
  # num_timesteps: 1000  # use defaults from code unless overridden elsewhere

optim:
  optimizer: adam
  lr: 1.0e-4

train:
  total_steps: 10000
  grad_clip: 1.0
  amp: true

loss:
  # Vanilla ε-MSE DDPM loss (no Min-SNR reweighting)
  weighting: "constant"

curvature:
  enabled: true
  method: "hutchinson"
  probes: 16
  log_prefix: "curvature/hutch"

eval:
  quick: false

  grid:
    enabled: true
    every: 5000
    sampler: ddim
    nfe: 20
    n_samples: 36
    save_images: true

  kid:
    enabled: false
    every: 4000
    n_samples: 1024
    repeats: 3
    sampler: ddim
    nfe: 20

  fid_milestone:
    enabled: true
    every: 2000        # 2k,4k,6k,8k,10k
    run_if_kid_improved_pct: 0.0
    sampler: ddpm
    nfe: 50
    n_samples: 5000
    fid_stats: "stats/cifar10_inception_train.npz"

  final:
    enabled: true
    at_end: true
    sampler: ddpm
    nfe: 50
    n_samples: 10000
    fid_stats: "stats/cifar10_inception_train.npz"

logging:
  enable: true
  backends: ["tensorboard", "wandb"]
  log_every_n_steps: 100

  wandb:
    mode: online
    project: min_snr
    run_name: "e7a-baseline-linear-hutch-10k-50nfe"
    tags:
      ["E7a", "baseline", "vanilla", "hutchinson-trace",
       "linear", "10k", "50nfe", "min_snr"]
    notes: "E7a: vanilla ε-MSE baseline rerun with Hutchinson trace (10k steps, 50 NFE FID)."

  tensorboard:
    flush_secs: 10